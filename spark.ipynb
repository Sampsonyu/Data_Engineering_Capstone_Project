{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Immigration Data in the United States\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This is the capstone project for the Udacity Data Engineering Nanodegree program. The idea is to take multiple disparate data sources, clean the data, and process it through an ETL pipeline to produce a usable data set for analytics.\n",
    "\n",
    "We will be looking at the immigration data for the U.S. ports (I94 immigration data). The plan is to enrich this data using the other data sources suggested, build an ETL pipeline to process the raw data and create a data warehouse which can be used for analytics. \n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import configparser\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear, quarter\n",
    "from pyspark.sql.functions import monotonically_increasing_id, round, substring, upper\n",
    "from pyspark.sql.types import *\n",
    "#import chart_studio.plotly as py\n",
    "import psycopg2\n",
    "\n",
    "# import sys  \n",
    "# sys.path.insert(0, \"../src/\")\n",
    "from etl import check_data_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Configuration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = configparser.ConfigParser()\n",
    "# config.read('config.cfg')\n",
    "\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-4PDH3NC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x286ec0cb608>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Project Scope\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "In this project, we will be gathering, assessing, cleaning, creating data models, setup and push data to a data warehouse where it will be analyzed to produce informative reports.\n",
    "\n",
    "At a high level:\n",
    "\n",
    "- Data is extracted from the immigration SAS data, partitioned by year, month, and day, and stored in a data lake on Amazon S3 as Parquet files.\n",
    "- The partitioned data is loaded into Redshift into staging tables\n",
    "- Design fact and dimension tables\n",
    "- The staging data is combined with other staged data sources to produce the final fact and dimension records in the Redshift warehouse.\n",
    "\n",
    "Example questions we could explore with the final data set:\n",
    "\n",
    "+ For a given port city, how many immigrants enter from which countries?\n",
    "+ Is there any relationship between the average temperature of the country of origin and average temperature of the port city of entry?\n",
    "+ Is there any relationship between the connection between the volume of travel and the number of entry ports (ie airports)\n",
    "+ The effects of temperature on the volume of travellers\n",
    "+ What time of year or month sees more immigration for certain areas?\n",
    "\n",
    "#### Technical Overview\n",
    "Project uses following technologies,\n",
    "\n",
    "+ AWS S3 Storage : To store inputs & outputs\n",
    "+ AWS Redshift as Date Warehouse for Analytics\n",
    "+ Juypter Notebooks\n",
    "+ Python\n",
    "+ Spark\n",
    "+ Libraries : Pandas & Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Datasets\n",
    "\n",
    "**I94 Immigration Data (immigration)** \n",
    "\n",
    "The i94 data contains information about visitors to the US via an i94 form that all visitors must complete. It comes from the US National Tourism and Trade Office and includes details on incoming immigrants and their ports of entry. - [source](https://www.trade.gov/national-travel-and-tourism-office). \n",
    "\n",
    "It is provided in SAS7BDAT format which is a binary database storage format. It is created by Statistical Analysis System (SAS) software to store data.\n",
    "\n",
    "The immigration data is partitioned into monthly SAS files. Each file is around 300 to 700 MB. The data provided represents 12 months of data for the year 2016. This is the bulk of the data used in the project.\n",
    "\n",
    "A data dictionary ```I94_SAS_Labels_Descriptions.SAS``` is provided for the immigration data. In addition to descriptions of the various fields, the port and country codes used were listed in table format. Two csv files are extracted from the dictionary: \n",
    "\n",
    "* ```i94_countries.csv```: Table containing country codes used in the dataset.\n",
    "* ```i94_ports.csv```: Table containing city codes used in the dataset.\n",
    "\n",
    "These files will be used as a lookup when extracting the immigration data.\n",
    "\n",
    "**World Temparature data (temperature)** \n",
    "\n",
    "This dataset comes from Kaggle. - [source](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "It contains average temperature data for countries and cities around the world between 1743-11-01 and 2013-09-01.\n",
    "\n",
    "The data is stored in ```GlobalLandTemperaturesByCity.csv``` (508 MB).\n",
    "\n",
    "**U.S. City Demographic Data (demographics)**\n",
    "\n",
    "This data comes from OpenSoft and contains demographic data for U.S. cities and states, such as age, population, veteran status and race. - [source](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "This data placed in the data lake as a single CSV file (246 KB). This data will be combined with port city data to provide ancillary demographic info for port cities.\n",
    "\n",
    "**Airport Code Table (airports)**\n",
    "\n",
    "This is a simple table of airport codes and corresponding cities. It comes from datahub.io. - [source](https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "This data is a single CSV file (5.8 KB). It provides additional information for airports and can be combined with the immigration port city info.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a table of datasets used in the project:\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Source name</th>\n",
    "<th>Filename</th>\n",
    "<th>Format</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>I94 Immigration Sample Data</td>\n",
    "<td>immigration_data_sample.csv</td>\n",
    "<td>csv</td>\n",
    "<td>This is a sample data which is from the US National Tourism and Trade Office.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://travel.trade.gov/research/reports/i94/historical/2016.html\">I94 Immigration Data</a></td>\n",
    "<td>data/18-83510-I94-Data-2016/i94_***16_sub.sas7bdat</td>\n",
    "<td>SAS</td>\n",
    "<td>This data comes from the US National Tourism and Trade Office.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\">World Temperature Data</a></td>\n",
    "<td>world_temperature.csv</td>\n",
    "<td>csv</td>\n",
    "<td>This dataset contains temperature data of various cities from 1700&#39;s - 2013. This dataset came from Kaggle.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\">U.S. City Demographic Data</a></td>\n",
    "<td>us-cities-demographics.csv</td>\n",
    "<td>csv</td>\n",
    "<td>This dataset contains population details of all US Cities and census-designated places includes gender &amp; race informatoin. This data came from OpenSoft.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://datahub.io/core/airport-codes#data\">Airport Codes</a></td>\n",
    "<td>airport-codes_csv.csv</td>\n",
    "<td>csv</td>\n",
    "<td>This is a simple table of airport codes and corresponding cities.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>I94_country</td>\n",
    "<td>i94_countries.csv</td>\n",
    "<td>csv</td>\n",
    "<td>Shows corresponding i94 Country of Citizenship &amp; Country of Residence codes. Source : I94_SAS_Labels_Descriptions.SAS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>I94_port</td>\n",
    "<td>i94_ports.csv</td>\n",
    "<td>csv</td>\n",
    "<td>Shows US Port of Entry city names and their corresponding codes. Source : I94_SAS_Labels_Descriptions.SAS</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data_path = '../Data_Engineering_Capstone_Project_local2/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration_spark =spark.read.format('com.github.saurfang.sas.spark').load(immigration_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  ...  entdepu  matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0      NaN  ...        U     None   1979.0  10282016   None   None    None   \n",
       "1      NaN  ...        Y     None   1991.0       D/S      M   None    None   \n",
       "2  20691.0  ...     None        M   1961.0  09302016      M   None      OS   \n",
       "3  20567.0  ...     None        M   1988.0  09302016   None   None      AA   \n",
       "4  20567.0  ...     None        M   2012.0  09302016   None   None      AA   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  1.897628e+09   None       B2  \n",
       "1  3.736796e+09  00296       F1  \n",
       "2  6.666432e+08     93       B2  \n",
       "3  9.246846e+10  00199       B2  \n",
       "4  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_immigration.info\n",
    "df_immigration_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+-------+------------------+------------------+-------+-----------------+------------------+------------------+------------------+------------------+-------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "|summary|            cicid|               i94yr| i94mon|            i94cit|            i94res|i94port|          arrdate|           i94mode|           i94addr|           depdate|            i94bir|            i94visa|  count|            dtadfile|visapost|            occup|entdepa|entdepd|entdepu|matflag|           biryear|           dtaddto| gender|           insnum|           airline|              admnum|             fltno|visatype|\n",
      "+-------+-----------------+--------------------+-------+------------------+------------------+-------+-----------------+------------------+------------------+------------------+------------------+-------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "|  count|          3096313|             3096313|3096313|           3096313|           3096313|3096313|          3096313|           3096074|           2943721|           2953856|           3095511|            3096313|3096313|             3096312| 1215063|             8126|3096075|2957884|    392|2957884|           3095511|           3095836|2682044|           113708|           3012686|             3096313|           3076764| 3096313|\n",
      "|   mean|3078651.879075533|              2016.0|    4.0| 304.9069344733559|303.28381949757664|   null|20559.84854179794|1.0736897761487614|51.652482269503544| 20573.95283554784|41.767614458485205| 1.8453925685161674|    1.0|2.0160424766168267E7|   999.0|          885.675|   null|   null|   null|   null|1974.2323855415148| 8291120.333841449|   null|4131.050016327899|59.477601493233784|7.082885011090295E10|1360.2463696420555|    null|\n",
      "| stddev|1763278.099749858|4.282829613261096...|    0.0|210.02688853063327| 208.5832129278886|   null|8.777339474881552|0.5158963131657236|42.979062313709846|29.356968481630613| 17.42026053458826|0.39839102005409577|    0.0|   50.01513449489737|     0.0|264.6551105950961|   null|   null|   null|   null|17.420260534588262|1656502.4244925014|   null|8821.743471773656|172.63339952061747|2.215441594755763...| 5852.676345633782|    null|\n",
      "|    min|              6.0|              2016.0|    4.0|             101.0|             101.0|    5KE|          20545.0|               1.0|                ..|           15176.0|              -3.0|                1.0|    1.0|            20130811|     999|              049|      A|      D|      U|      M|            1902.0|          /   183D|      F|                0|               *FF|                 0.0|             00000|      B1|\n",
      "|    max|        6102785.0|              2016.0|    4.0|             999.0|             760.0|    YSL|          20574.0|               9.0|                ZU|           45427.0|             114.0|                3.0|    1.0|            20160919|     ZZZ|              WTR|      Z|      W|      Y|      M|            2019.0|               D/S|      X|           YM0167|                ZZ|      9.991556593E10|               ZZZ|      WT|\n",
      "+-------+-----------------+--------------------+-------+------------------+------------------+-------+-----------------+------------------+------------------+------------------+------------------+-------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the whole sad7bdat files sperated by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, dtadfile: string, visapost: string, entdepa: string, biryear: double, gender: string, airline: string, fltno: string, visatype: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns with over 90% missing values and with no sense \n",
    "# df_immigration.drop(columns = ['occup', 'entdepu','insnum', 'count', 'dtaddto', 'entdepd', 'admnum', 'matflag'])\n",
    "df_immigration_spark.drop('occup', 'entdepu','insnum', 'count', 'dtaddto', 'entdepd', 'admnum', 'matflag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create udf to convert SAS date to PySpark date \n",
    "# @udf(StringType())\n",
    "# def convert_datetime(x):\n",
    "#     if x:\n",
    "#         return (datetime.datetime(1960, 1, 1).date() + datetime.timedelta(x)).day\n",
    "#     return None\n",
    "# # return (datetime.datetime(1960, 1, 1).date() + datetime.timedelta(x)).isoformat()\n",
    "df_immigration_cleaned = df_immigration_spark.dropDuplicates(['cicid'])\n",
    "df_immigration_cleaned = df_immigration_spark.dropna(how=\"any\", subset=[\"i94port\", \"i94addr\", \"gender\"])\n",
    "#df_immigration_cleaned = df_immigration_cleaned.withColumn(\"arrdate\", convert_datetime(df_immigration_cleaned.arrdate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact ports info from I94_SAS_Labels_Descriptions.SAS\n",
    "i94_ports.csv: Table containing city codes used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF for process date, mode, visa\n",
    "def convert_sas_date(days):\n",
    "    \"\"\"\n",
    "    Converts SAS date stored as days since 1/1/1960 to datetime\n",
    "    :param days: Days since 1/1/1960\n",
    "    :return: datetime\n",
    "    \"\"\"\n",
    "    if days is None:\n",
    "        return None\n",
    "    return datetime.date(1960, 1, 1) + datetime.timedelta(days=days)\n",
    "\n",
    "\n",
    "def get_sas_day(days):\n",
    "    \"\"\"\n",
    "    Converts SAS date stored as days since 1/1/1960 to day of month\n",
    "    :param days: Days since 1/1/1960\n",
    "    :return: Day of month value as integer\n",
    "    \"\"\"\n",
    "    if days is None:\n",
    "        return None\n",
    "    return (datetime.date(1960, 1, 1) + datetime.timedelta(days=days)).day\n",
    "\n",
    "\n",
    "def convert_i94mode(mode):\n",
    "    \"\"\"\n",
    "    Converts i94 travel mode code to a description\n",
    "    :param mode: int i94 mode as integer\n",
    "    :return: i94 mode description\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        return \"Air\"\n",
    "    elif mode == 2:\n",
    "        return \"Sea\"\n",
    "    elif mode == 3:\n",
    "        return \"Land\"\n",
    "    else:\n",
    "        return \"Not Reported\"\n",
    "\n",
    "\n",
    "def convert_visa(visa):\n",
    "    \"\"\"\n",
    "    Converts visa numeric code to description\n",
    "    :param visa: str\n",
    "    :return: Visa description: str\n",
    "    \"\"\"\n",
    "    if visa is None:\n",
    "        return \"Not Reported\"\n",
    "    elif visa == 1:\n",
    "        return \"Business\"\n",
    "    elif visa == 2:\n",
    "        return \"Pleasure\"\n",
    "    elif visa == 3:\n",
    "        return \"Student\"\n",
    "    else:\n",
    "        return \"Not Reported\"\n",
    "\n",
    "\n",
    "def format_state(s):\n",
    "    \"\"\"\n",
    "    Format state column\n",
    "    :param s:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s = s.replace('DIST. OF', 'District of') \\\n",
    "         .replace('S.', 'South') \\\n",
    "         .replace('N.', 'North') \\\n",
    "         .replace('W.', 'West')\n",
    "    return ' '.join([w.capitalize() if w != 'of' else w for w in s.split()])\n",
    "\n",
    "\n",
    "def covert_i94port(i94port, i94addr):\n",
    "    \"\"\"\n",
    "    Process i94port dictionary\n",
    "    :param i94port:\n",
    "    :param i94addr:\n",
    "    :return: i94port_split\n",
    "    \"\"\"\n",
    "    i94port_split = {}\n",
    "    index = 0\n",
    "    for k, v in i94port.items():\n",
    "        if not re.match('^Collapsed|^No PORT Code', v):\n",
    "            try:\n",
    "                # extract state part from i94port\n",
    "                # the state part contains the state and also other words\n",
    "                state_part = v.rsplit(',', 1)[1]\n",
    "                city_part = v.rsplit(',', 1)[0]\n",
    "                # create a set of all words in state part\n",
    "                state_part_set = set(state_part.split())\n",
    "                # if the state is valid (is in the set(i94addr.keys()), then retrieve state\n",
    "                state = list(set(i94addr.keys()).intersection(state_part_set))[0]\n",
    "                # add state to dict\n",
    "                i94port_split[index] = [k, city_part, state]\n",
    "            except IndexError:\n",
    "                # no state is specified for Washington DC in labels so it is added here\n",
    "                # 'MARIPOSA AZ' is not split by \",\"\n",
    "                if v == 'WASHINGTON DC':\n",
    "                    i94port_split[index] = [k, 'WASHINGTON DC', 'DC']\n",
    "                elif v == 'MARIPOSA AZ':\n",
    "                    i94port_split[index] = [k, 'MARIPOSA', 'AZ']\n",
    "                else:\n",
    "                    i94port_split[index] = [k, 'NULL', 'NULL']\n",
    "\n",
    "        else:\n",
    "            i94port_split[index] = [k, 'NULL', 'NULL']\n",
    "        index += 1\n",
    "    return i94port_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_airport_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Process the airport data files, transform it to airport table\n",
    "    :param spark: spark session\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "   \"\"\"\n",
    "    print('Processing airport codes dataset...')\n",
    "\n",
    "    # get filepath to airport data file\n",
    "    # airport_data_path = os.path.join(input_data, 'airport-codes_csv.csv')\n",
    "    airport_data_path = input_data + 'airport-codes_csv.csv'\n",
    "    df_airport_spark = spark.read\\\n",
    "        .format('csv')\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .load(airport_data_path)\\\n",
    "        .drop(\"coordinates\", \"gps_code\", \"continent\", \"elevation_ft\")\n",
    "\n",
    "    dim_airport = df_airport_spark\\\n",
    "        .filter((col(\"type\") == \"small_airport\") |\n",
    "                (col(\"type\") == \"large_airport\") |\n",
    "                (col(\"type\") == \"medium_airport\"))\\\n",
    "        .filter(df_airport_spark[\"iso_country\"] == \"US\")\\\n",
    "        .filter(df_airport_spark['local_code'].isNotNull())\\\n",
    "        .withColumn(\"state\", substring(df_airport_spark[\"iso_region\"], 4, 2))\n",
    "#         .filter(df_airport_spark['iata_code'].isNotNull()) \\\n",
    "\n",
    "    # airport_out_path = os.path.join(output_data, 'dim_airport/')\n",
    "    airport_out_path = output_data + 'dim_airport/'\n",
    "    dim_airport.write.parquet(airport_out_path)\n",
    "\n",
    "    print('Finished processing airport codes data.')\n",
    "    # return dim_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_i94mode_udf = udf(convert_i94mode, StringType())\n",
    "convert_sas_date_udf = udf(convert_sas_date, DateType())\n",
    "convert_visa_udf = udf(convert_visa, StringType())\n",
    "get_sas_day_udf = udf(get_sas_day, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-------+-----+---+-------+----+\n",
      "|time_id|      date|year|quarter|month|day|weekday|week|\n",
      "+-------+----------+----+-------+-----+---+-------+----+\n",
      "|  20593|2016-05-19|2016|      2|    5| 19|      5|  20|\n",
      "|  20689|2016-08-23|2016|      3|    8| 23|      3|  34|\n",
      "|  20673|2016-08-07|2016|      3|    8|  7|      1|  31|\n",
      "|  20467|2016-01-14|2016|      1|    1| 14|      5|   2|\n",
      "|  20652|2016-07-17|2016|      3|    7| 17|      1|  28|\n",
      "|  20196|2015-04-18|2015|      2|    4| 18|      7|  16|\n",
      "|  20705|2016-09-08|2016|      3|    9|  8|      5|  36|\n",
      "|  20621|2016-06-16|2016|      2|    6| 16|      5|  24|\n",
      "|  20550|2016-04-06|2016|      2|    4|  6|      4|  14|\n",
      "|  20682|2016-08-16|2016|      3|    8| 16|      3|  33|\n",
      "|  20203|2015-04-25|2015|      2|    4| 25|      7|  17|\n",
      "|  20592|2016-05-18|2016|      2|    5| 18|      4|  20|\n",
      "|  20614|2016-06-09|2016|      2|    6|  9|      5|  23|\n",
      "|  20556|2016-04-12|2016|      2|    4| 12|      3|  15|\n",
      "|  20683|2016-08-17|2016|      3|    8| 17|      4|  33|\n",
      "|  20518|2016-03-05|2016|      1|    3|  5|      7|   9|\n",
      "|  20553|2016-04-09|2016|      2|    4|  9|      7|  14|\n",
      "|  20622|2016-06-17|2016|      2|    6| 17|      6|  24|\n",
      "|  20527|2016-03-14|2016|      1|    3| 14|      2|  11|\n",
      "|  20699|2016-09-02|2016|      3|    9|  2|      6|  35|\n",
      "+-------+----------+----+-------+-----+---+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dim_time spark dataframe\n",
    "df_time_spark = df_immigration_spark.select(col(\"arrdate\").alias(\"time\")).union(df_immigration_spark.select(col(\"depdate\").alias(\"time\"))).distinct()\n",
    "# df_time_spark.show()\n",
    "time_table = df_time_spark.withColumn('date', convert_sas_date_udf(df_time_spark['time']))\n",
    "time_table = time_table.select(\n",
    "    col('time').alias('time_id').cast(IntegerType())\\\n",
    "    , col('date').alias('date') \\\n",
    "    , year('date').alias('year') \\\n",
    "    , quarter('date').alias('quarter')\\\n",
    "    , month('date').alias('month') \\\n",
    "    , dayofmonth('date').alias('day') \\\n",
    "    , dayofweek('date').alias('weekday')\\\n",
    "    , weekofyear('date').alias('week'))\n",
    "time_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_spark = df_immigration_spark \\\n",
    "        .withColumn('arrival_date', convert_sas_date_udf(df_immigration_spark['arrdate'])) \\\n",
    "        .withColumn('departure_date', convert_sas_date_udf(df_immigration_spark['depdate'])) \\\n",
    "        .withColumn('arrival_year', df_immigration_spark['i94yr'].cast(IntegerType())) \\\n",
    "        .withColumn('arrival_month', df_immigration_spark['i94mon'].cast(IntegerType())) \\\n",
    "        .withColumn('arrival_day', get_sas_day_udf(df_immigration_spark['arrdate'])) \\\n",
    "        .withColumn('age', df_immigration_spark['i94bir'].cast(IntegerType())) \\\n",
    "        .withColumn('country_of_bir', df_immigration_spark['i94cit'].cast(IntegerType())) \\\n",
    "        .withColumn('country_of_res', df_immigration_spark['i94res'].cast(IntegerType())) \\\n",
    "        .withColumn('port_of_admission', df_immigration_spark['i94port'].cast(StringType())) \\\n",
    "        .withColumn('birth_year', df_immigration_spark['biryear'].cast(IntegerType())) \\\n",
    "        .withColumn('mode', convert_i94mode_udf(df_immigration_spark['i94mode'])) \\\n",
    "        .withColumn('visa_category', convert_visa_udf(df_immigration_spark['i94visa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immigration = df_immigration_spark \\\n",
    "            .filter(df_immigration_spark['country_of_bir'].isNotNull()) \\\n",
    "            .filter(df_immigration_spark['country_of_res'].isNotNull()) \\\n",
    "            .filter(df_immigration_spark['arrdate'].isNotNull()) \\\n",
    "            .filter(df_immigration_spark['depdate'].isNotNull()) \\\n",
    "            .filter(df_immigration_spark['port_of_admission'].isNotNull()) \\\n",
    "            .select(\n",
    "                col('cicid').alias('id'),\n",
    "                col('arrdate').alias('arr_ts'),\n",
    "                'arrival_year',\n",
    "                'arrival_month',\n",
    "                'country_of_bir',\n",
    "                'country_of_res',\n",
    "                'port_of_admission',\n",
    "                'mode',\n",
    "                'visa_category',\n",
    "                'visatype',\n",
    "                col('depdate').alias('dep_ts')) \\\n",
    "            .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immigration = df_immigration_spark.select(\n",
    "        col('cicid').alias('id').cast(IntegerType()),\n",
    "        col('arrdate').alias('arrdate_id').cast(IntegerType()),\n",
    "        col('depdate').alias('depdate_id').cast(IntegerType()),\n",
    "        'country_of_bir',\n",
    "        'country_of_res',\n",
    "        'port_of_admission',\n",
    "        'mode',\n",
    "        'visa_category',\n",
    "        'visatype') \\\n",
    "        .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- arrdate_id: integer (nullable = true)\n",
      " |-- depdate_id: integer (nullable = true)\n",
      " |-- country_of_bir: integer (nullable = true)\n",
      " |-- country_of_res: integer (nullable = true)\n",
      " |-- port_of_admission: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigration.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arrdate_id</th>\n",
       "      <th>depdate_id</th>\n",
       "      <th>country_of_bir</th>\n",
       "      <th>country_of_res</th>\n",
       "      <th>port_of_admission</th>\n",
       "      <th>mode</th>\n",
       "      <th>visa_category</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>20545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218</td>\n",
       "      <td>20545</td>\n",
       "      <td>20548.0</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420</td>\n",
       "      <td>20545</td>\n",
       "      <td>20601.0</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1153</td>\n",
       "      <td>20545</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>OGG</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1221</td>\n",
       "      <td>20545</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  arrdate_id  depdate_id  country_of_bir  country_of_res  \\\n",
       "0   212       20545         NaN             103             103   \n",
       "1   218       20545     20548.0             103             103   \n",
       "2   420       20545     20601.0             103             103   \n",
       "3  1153       20545     20554.0             104             104   \n",
       "4  1221       20545     20546.0             104             104   \n",
       "\n",
       "  port_of_admission mode visa_category visatype  \n",
       "0               NYC  Air      Pleasure       WT  \n",
       "1               NYC  Air      Pleasure       WT  \n",
       "2               NYC  Air      Pleasure       WT  \n",
       "3               OGG  Air      Pleasure       WT  \n",
       "4               NYC  Air      Pleasure       WT  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_cleaned.createOrReplaceTempView(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT admnum)|\n",
      "+----------------------+\n",
      "|               2540211|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT(admnum))\n",
    "    FROM immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              2551402|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT(cicid))\n",
    "    FROM immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2953856"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All invalid values are grouped together as 'INVALID ENTRY'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94cntyl = { k: (v if not re.match('^INVALID:|^Collapsed|^No Country Code', v) else 'INVALID ENTRY') \n",
    "                 for k, v in i94cntyl.items()}\n",
    "#i94cntyl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states in map are formatted to comply with state names in other datasets.\n",
    "\n",
    "+ 'DIST. OF' is replaced with 'District of'\n",
    "+ 'S.', 'N.', 'W.' are replaced with 'South', 'North', 'West'\n",
    "+ all states are capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(s):\n",
    "    s = s.replace('DIST. OF', 'District of') \\\n",
    "         .replace('S.', 'South') \\\n",
    "         .replace('N.', 'North') \\\n",
    "         .replace('W.', 'West')\n",
    "    return ' '.join([w.capitalize() if w != 'of' else w for w in s.split() ])\n",
    "\n",
    "# format addr labels\n",
    "i94addr = {k: format_state(v) for k, v in i94addr.items()}\n",
    "# i94addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94port_split = {}\n",
    "index = 0 \n",
    "for k, v in i94port.items():\n",
    "    if not re.match('^Collapsed|^No PORT Code', v):\n",
    "        try:\n",
    "            # extract state part from i94port\n",
    "            # the state part contains the state and also other words\n",
    "            state_part = v.rsplit(',', 1)[1]\n",
    "            city_part = v.rsplit(',', 1)[0] \n",
    "            # create a set of all words in state part\n",
    "            state_part_set = set(state_part.split())\n",
    "            # if the state is valid (is in the set(i94addr.keys()), then retrieve state\n",
    "            state = list(set(i94addr.keys()).intersection(state_part_set))[0]\n",
    "            # add state to dict\n",
    "            i94port_split[index] = [k, city_part, state]\n",
    "        except IndexError:\n",
    "            # no state is specified for Washington DC in labels so it is added here\n",
    "            # 'MARIPOSA AZ' is not split by \",\"\n",
    "            if v == 'WASHINGTON DC':\n",
    "                i94port_split[index] = [k, 'WASHINGTON DC','DC']\n",
    "            elif v == 'MARIPOSA AZ':\n",
    "                i94port_split[index] = [k,  'MARIPOSA', 'AZ']\n",
    "            else:\n",
    "                i94port_split[index] = [k, 'INVALID ENTRY', 'INVALID ENTRY']\n",
    "            \n",
    "    else:\n",
    "        i94port_split[index] = [k, 'INVALID ENTRY', 'INVALID ENTRY']\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94cntyl = pd.DataFrame(i94cntyl.items(), columns=['country_code', 'country'])\n",
    "#df_i94port = pd.DataFrame(i94port.items(), columns=['port_code', 'port'])\n",
    "df_i94port = pd.DataFrame(i94port_split.values(), columns=['port_code', 'city_name', 'state_code'])\n",
    "df_i94mode = pd.DataFrame(i94mode.items(), columns=['mode_num', 'mode'])\n",
    "df_i94addr = pd.DataFrame(i94addr.items(), columns=['state_id', 'state_name'])\n",
    "df_i94visa = pd.DataFrame(i94visa.items(), columns=['visa_type_num', 'visa_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94port_spark = spark.createDataFrame(df_i94port)\n",
    "df_i94addr_spark = spark.createDataFrame(df_i94addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94port_dict = df_i94port_spark\\\n",
    "    .join(df_i94addr_spark, df_i94port_spark['state_code'] == df_i94addr_spark['state_id'], 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94port_dict = df_i94port_dict.select(\"port_code\", upper(col(\"city_name\")).alias(\"city_name\"), \"state_code\", upper(col(\"state_name\")).alias(\"state_name\"))\\\n",
    "                 .filter(df_i94port_dict[\"state_name\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+--------------+\n",
      "|port_code|   city_name|state_code|    state_name|\n",
      "+---------+------------+----------+--------------+\n",
      "|      DOU|     DOUGLAS|        AZ|       ARIZONA|\n",
      "|      LUK|   LUKEVILLE|        AZ|       ARIZONA|\n",
      "|      MAP|    MARIPOSA|        AZ|       ARIZONA|\n",
      "|      NAC|        NACO|        AZ|       ARIZONA|\n",
      "|      NOG|     NOGALES|        AZ|       ARIZONA|\n",
      "|      PHO|     PHOENIX|        AZ|       ARIZONA|\n",
      "|      POR|      PORTAL|        AZ|       ARIZONA|\n",
      "|      SLU|    SAN LUIS|        AZ|       ARIZONA|\n",
      "|      SAS|      SASABE|        AZ|       ARIZONA|\n",
      "|      TUC|      TUCSON|        AZ|       ARIZONA|\n",
      "|      YUI|        YUMA|        AZ|       ARIZONA|\n",
      "|      YUM|        YUMA|        AZ|       ARIZONA|\n",
      "|      CHL|  CHARLESTON|        SC|SOUTH CAROLINA|\n",
      "|      CAE|    COLUMBIA|        SC|SOUTH CAROLINA|\n",
      "|      GEO|  GEORGETOWN|        SC|SOUTH CAROLINA|\n",
      "|      GSP|  GREENVILLE|        SC|SOUTH CAROLINA|\n",
      "|      GRR|       GREER|        SC|SOUTH CAROLINA|\n",
      "|      MYR|MYRTLE BEACH|        SC|SOUTH CAROLINA|\n",
      "|      BTN| BATON ROUGE|        LA|     LOUISIANA|\n",
      "|      LKC|LAKE CHARLES|        LA|     LOUISIANA|\n",
      "+---------+------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94port_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94port_dict.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_i94cntyl.to_csv('../Data/i94cntyl.csv', index=False, sep=',')\n",
    "df_i94port.to_csv('../Data/i94port.csv', index=False, sep=',')\n",
    "df_i94mode.to_csv('../Data/i94mode.csv', index=False, sep=',')\n",
    "df_i94addr.to_csv('../Data/i94addr.csv', index=False, sep=',')\n",
    "df_i94visa.to_csv('../Data/i94visa.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of valid i94port codes is created\n",
    "i94_sas_label_descriptions_fname = \"data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "with open(i94_sas_label_descriptions_fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "re_compiled = re.compile(r\"\\'(.*)\\'.*\\'(.*)\\'\")\n",
    "valid_ports = {}\n",
    "for line in lines[302:961]:\n",
    "    results = re_compiled.search(line)\n",
    "    valid_ports[results.group(1)] = results.group(2)\n",
    "print(len(valid_ports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immigration = dim_immigration.join(df_i94port_dict, \n",
    "          df_i94port_dict['port_code'] == dim_immigration['port_of_admission'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arrdate_id</th>\n",
       "      <th>depdate_id</th>\n",
       "      <th>country_of_bir</th>\n",
       "      <th>country_of_res</th>\n",
       "      <th>port_of_admission</th>\n",
       "      <th>mode</th>\n",
       "      <th>visa_category</th>\n",
       "      <th>visatype</th>\n",
       "      <th>port_code</th>\n",
       "      <th>city_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3606841</td>\n",
       "      <td>20563</td>\n",
       "      <td>20575</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>BGM</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>B2</td>\n",
       "      <td>BGM</td>\n",
       "      <td>BANGOR</td>\n",
       "      <td>ME</td>\n",
       "      <td>MAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3599392</td>\n",
       "      <td>20563</td>\n",
       "      <td>20575</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>BGM</td>\n",
       "      <td>Air</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>B2</td>\n",
       "      <td>BGM</td>\n",
       "      <td>BANGOR</td>\n",
       "      <td>ME</td>\n",
       "      <td>MAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2177010</td>\n",
       "      <td>20556</td>\n",
       "      <td>20585</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>BGM</td>\n",
       "      <td>Air</td>\n",
       "      <td>Business</td>\n",
       "      <td>B1</td>\n",
       "      <td>BGM</td>\n",
       "      <td>BANGOR</td>\n",
       "      <td>ME</td>\n",
       "      <td>MAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4671460</td>\n",
       "      <td>20569</td>\n",
       "      <td>20571</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>BGM</td>\n",
       "      <td>Air</td>\n",
       "      <td>Business</td>\n",
       "      <td>B1</td>\n",
       "      <td>BGM</td>\n",
       "      <td>BANGOR</td>\n",
       "      <td>ME</td>\n",
       "      <td>MAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4040457</td>\n",
       "      <td>20566</td>\n",
       "      <td>20581</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>BGM</td>\n",
       "      <td>Air</td>\n",
       "      <td>Business</td>\n",
       "      <td>B1</td>\n",
       "      <td>BGM</td>\n",
       "      <td>BANGOR</td>\n",
       "      <td>ME</td>\n",
       "      <td>MAINE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  arrdate_id  depdate_id  country_of_bir  country_of_res  \\\n",
       "0  3606841       20563       20575             582             582   \n",
       "1  3599392       20563       20575             582             582   \n",
       "2  2177010       20556       20585             135             135   \n",
       "3  4671460       20569       20571             108             108   \n",
       "4  4040457       20566       20581             111             111   \n",
       "\n",
       "  port_of_admission mode visa_category visatype port_code city_name  \\\n",
       "0               BGM  Air      Pleasure       B2       BGM    BANGOR   \n",
       "1               BGM  Air      Pleasure       B2       BGM    BANGOR   \n",
       "2               BGM  Air      Business       B1       BGM    BANGOR   \n",
       "3               BGM  Air      Business       B1       BGM    BANGOR   \n",
       "4               BGM  Air      Business       B1       BGM    BANGOR   \n",
       "\n",
       "  state_code state_name  \n",
       "0         ME      MAINE  \n",
       "1         ME      MAINE  \n",
       "2         ME      MAINE  \n",
       "3         ME      MAINE  \n",
       "4         ME      MAINE  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dim_immigration = dim_immigration\\\n",
    "    .select('id', 'arrdate_id', 'depdate_id','country_of_bir','country_of_res','port_of_admission','mode','visa_category','visatype', 'city_name','state_name')\\\n",
    "    .join(df_demographics_spark_pivot, (df_demographics_spark_pivot['city'] == dim_immigration['city_name']) & (df_demographics_spark_pivot['state'] == dim_immigration['state_name']) , 'inner')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dim_immigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. World Temparature Data "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Spark\n",
    "temperature_data_path = 'data/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature_data_spark = spark.read.csv(temperature_data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data_path = 'data/GlobalLandTemperaturesByCity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature = pd.read_csv(temperature_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                object\n",
       "AverageTemperature               float64\n",
       "AverageTemperatureUncertainty    float64\n",
       "City                              object\n",
       "Country                           object\n",
       "Latitude                          object\n",
       "Longitude                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_spark = spark.read.csv(temperature_data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'timestamp'),\n",
       " ('AverageTemperature', 'double'),\n",
       " ('AverageTemperatureUncertainty', 'double'),\n",
       " ('City', 'string'),\n",
       " ('Country', 'string'),\n",
       " ('Latitude', 'string'),\n",
       " ('Longitude', 'string')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_spark = df_temperature_spark \\\n",
    "    .filter(df_temperature_spark[\"country\"] == \"United States\") \\\n",
    "    .filter(df_temperature_spark.AverageTemperature.isNotNull())\\\n",
    "    .filter(year(df_temperature_spark[\"dt\"]) == 2012)\\\n",
    "    .withColumn(\"year\", year(df_temperature_spark[\"dt\"])) \\\n",
    "    .withColumn(\"month\", month(df_temperature_spark[\"dt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'timestamp'),\n",
       " ('AverageTemperature', 'double'),\n",
       " ('AverageTemperatureUncertainty', 'double'),\n",
       " ('City', 'string'),\n",
       " ('Country', 'string'),\n",
       " ('Latitude', 'string'),\n",
       " ('Longitude', 'string'),\n",
       " ('year', 'int'),\n",
       " ('month', 'int')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_table = df_temperature_spark.select(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"AverageTemperature\",\n",
    "    upper(col(\"City\")).alias('city'),\n",
    "    \"Country\")\\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>city</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2.382</td>\n",
       "      <td>ALEXANDRIA</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>26.349</td>\n",
       "      <td>AUSTIN</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>7.129</td>\n",
       "      <td>CARY</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>27.427</td>\n",
       "      <td>CHATTANOOGA</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1.023</td>\n",
       "      <td>CINCINNATI</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  AverageTemperature         city        Country\n",
       "0  2012      1               2.382   ALEXANDRIA  United States\n",
       "1  2012      9              26.349       AUSTIN  United States\n",
       "2  2012      2               7.129         CARY  United States\n",
       "3  2012      7              27.427  CHATTANOOGA  United States\n",
       "4  2012      1               1.023   CINCINNATI  United States"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature_table.toPandas().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_table = temperature_table.groupby(\"year\", \"city\").avg(\"AverageTemperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>city</th>\n",
       "      <th>avg(AverageTemperature)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>TORRANCE</td>\n",
       "      <td>17.089583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>INGLEWOOD</td>\n",
       "      <td>17.089583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>FORT WAYNE</td>\n",
       "      <td>12.203833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>FORT COLLINS</td>\n",
       "      <td>10.561417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>OAKLAND</td>\n",
       "      <td>15.051583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year          city  avg(AverageTemperature)\n",
       "0  2012      TORRANCE                17.089583\n",
       "1  2012     INGLEWOOD                17.089583\n",
       "2  2012    FORT WAYNE                12.203833\n",
       "3  2012  FORT COLLINS                10.561417\n",
       "4  2012       OAKLAND                15.051583"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_by_city = temperature_table.select(\"year\", \"city\", round(col(\"avg(AverageTemperature)\"),2).alias(\"avg_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>city</th>\n",
       "      <th>avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>TORRANCE</td>\n",
       "      <td>17.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>INGLEWOOD</td>\n",
       "      <td>17.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>FORT WAYNE</td>\n",
       "      <td>12.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>FORT COLLINS</td>\n",
       "      <td>10.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>OAKLAND</td>\n",
       "      <td>15.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year          city  avg_temp\n",
       "0  2012      TORRANCE     17.09\n",
       "1  2012     INGLEWOOD     17.09\n",
       "2  2012    FORT WAYNE     12.20\n",
       "3  2012  FORT COLLINS     10.56\n",
       "4  2012       OAKLAND     15.05"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_by_city.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_by_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>city_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOU</td>\n",
       "      <td>DOUGLAS</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LUK</td>\n",
       "      <td>LUKEVILLE</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAP</td>\n",
       "      <td>MARIPOSA</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAC</td>\n",
       "      <td>NACO</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOG</td>\n",
       "      <td>NOGALES</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code  city_name state_code state_name\n",
       "0       DOU    DOUGLAS         AZ    ARIZONA\n",
       "1       LUK  LUKEVILLE         AZ    ARIZONA\n",
       "2       MAP   MARIPOSA         AZ    ARIZONA\n",
       "3       NAC       NACO         AZ    ARIZONA\n",
       "4       NOG    NOGALES         AZ    ARIZONA"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94port_dict.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94port_dict_with_temp = df_i94port_dict\\\n",
    "    .join(temperature_by_city, df_i94port_dict['city_name'] == temperature_by_city['City'], 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94port_dict_with_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>city_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "      <th>year</th>\n",
       "      <th>city</th>\n",
       "      <th>avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADW</td>\n",
       "      <td>ANDREWS AFB</td>\n",
       "      <td>MD</td>\n",
       "      <td>MARYLAND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANZ</td>\n",
       "      <td>ANZALDUAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAV</td>\n",
       "      <td>SAVANNAH</td>\n",
       "      <td>GA</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>SAVANNAH</td>\n",
       "      <td>20.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAR</td>\n",
       "      <td>CARIBOU MUNICIPAL AIRPORT</td>\n",
       "      <td>MN</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code                  city_name state_code state_name    year  \\\n",
       "0       ADW                ANDREWS AFB         MD   MARYLAND     NaN   \n",
       "1       ANZ                  ANZALDUAS         TX      TEXAS     NaN   \n",
       "2       BAR   BAKER AAF - BAKER ISLAND         AK     ALASKA     NaN   \n",
       "3       SAV                   SAVANNAH         GA    GEORGIA  2012.0   \n",
       "4       CAR  CARIBOU MUNICIPAL AIRPORT         MN  MINNESOTA     NaN   \n",
       "\n",
       "       city  avg_temp  \n",
       "0      None       NaN  \n",
       "1      None       NaN  \n",
       "2      None       NaN  \n",
       "3  SAVANNAH     20.78  \n",
       "4      None       NaN  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94port_dict_with_temp.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. US City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_data_path = \"data/us-cities-demographics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_spark = spark.read.csv(demographics_data_path, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate percentages of each numeric column and create new columns.\n",
    "df_demographics_spark=df_demographics_spark\\\n",
    ".withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    ".withColumn(\"pct_male_pop\",df_demographics_spark[\"Male Population\"]/df_demographics_spark[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_female_pop\",df_demographics_spark[\"Female Population\"]/df_demographics_spark[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_veterans\",df_demographics_spark[\"Number of Veterans\"]/df_demographics_spark[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_foreign_born\",df_demographics_spark[\"Foreign-born\"]/df_demographics_spark[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_race\",df_demographics_spark[\"Count\"]/df_demographics_spark[\"Total Population\"]*100)\\\n",
    ".orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns with new calculated percentages.\n",
    "df_demographics_spark_select = df_demographics_spark\\\n",
    "    .select(col(\"City\").alias(\"city\"),\\\n",
    "            col(\"State\").alias(\"state\"), \\\n",
    "            col(\"Median Age\").alias(\"median_age\"),\\\n",
    "            \"pct_male_pop\",\\\n",
    "            \"pct_female_pop\",\\\n",
    "            \"pct_veterans\",\\\n",
    "            \"pct_foreign_born\",\\\n",
    "            \"Race\",\\\n",
    "            \"pct_race\",\\\n",
    "            \"Total Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot the Race column\n",
    "df_demographics_spark_pivot = df_demographics_spark_select.groupBy(\"city\", \"state\", \"median_age\", \"pct_male_pop\",\\\n",
    "                                    \"pct_female_pop\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\", \"Total Population\").pivot(\"Race\").avg(\"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the header name of the race fields for spark compatibility.\n",
    "df_demographics_spark_pivot=df_demographics_spark_pivot\\\n",
    "    .select(upper(col(\"city\")).alias(\"city\"),\n",
    "            upper(col(\"state\")).alias(\"state\"),\n",
    "            round(col(\"median_age\")).alias(\"median_age\"),\n",
    "             round(col(\"pct_male_pop\"), 1).alias(\"perc_of_male_pop\"),\n",
    "             round(col(\"pct_female_pop\"), 1).alias(\"perc_of_female_pop\"),\n",
    "             round(col(\"pct_veterans\"), 1).alias(\"perc_of_veterans\"),\n",
    "             round(col(\"pct_foreign_born\"), 1).alias(\"perc_of_born\"),\\\n",
    "             round(col(\"American Indian and Alaska Native\"), 1).alias(\"perc_of_native_american\"),\\\n",
    "             round(col(\"Asian\"), 1).alias(\"perc_of_asian\"),\\\n",
    "             round(col(\"Black or African-American\"), 1).alias(\"perc_of_black\"),\\\n",
    "             round(col(\"Hispanic or Latino\"), 1).alias(\"perc_of_latino\"),\\\n",
    "             round(col(\"White\"), 1).alias(\"perc_of_white\"),\\\n",
    "             col(\"Total Population\").alias(\"total_pop\"))\\\n",
    "    .dropDuplicates(['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_spark_pivot.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_spark_pivot = df_demographics_spark_pivot.withColumn(\"city_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>perc_of_male_pop</th>\n",
       "      <th>perc_of_female_pop</th>\n",
       "      <th>perc_of_veterans</th>\n",
       "      <th>perc_of_born</th>\n",
       "      <th>perc_of_native_american</th>\n",
       "      <th>perc_of_asian</th>\n",
       "      <th>perc_of_black</th>\n",
       "      <th>perc_of_latino</th>\n",
       "      <th>perc_of_white</th>\n",
       "      <th>total_pop</th>\n",
       "      <th>city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHINO</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.0</td>\n",
       "      <td>59.6</td>\n",
       "      <td>40.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>15.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>85599</td>\n",
       "      <td>8589934592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAMFORD</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.4</td>\n",
       "      <td>49.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>25.8</td>\n",
       "      <td>66.4</td>\n",
       "      <td>128877</td>\n",
       "      <td>8589934593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YAKIMA</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>34.0</td>\n",
       "      <td>48.2</td>\n",
       "      <td>51.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>84.6</td>\n",
       "      <td>93700</td>\n",
       "      <td>8589934594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMARILLO</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>34.0</td>\n",
       "      <td>49.8</td>\n",
       "      <td>50.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.8</td>\n",
       "      <td>87.3</td>\n",
       "      <td>199651</td>\n",
       "      <td>17179869184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUNCIE</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>27.0</td>\n",
       "      <td>46.6</td>\n",
       "      <td>53.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>86.0</td>\n",
       "      <td>69701</td>\n",
       "      <td>17179869185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city        state  median_age  perc_of_male_pop  perc_of_female_pop  \\\n",
       "0     CHINO   CALIFORNIA        37.0              59.6                40.4   \n",
       "1  STAMFORD  CONNECTICUT        35.0              50.4                49.6   \n",
       "2    YAKIMA   WASHINGTON        34.0              48.2                51.8   \n",
       "3  AMARILLO        TEXAS        34.0              49.8                50.2   \n",
       "4    MUNCIE      INDIANA        27.0              46.6                53.4   \n",
       "\n",
       "   perc_of_veterans  perc_of_born  perc_of_native_american  perc_of_asian  \\\n",
       "0               4.9          21.8                      5.6           15.1   \n",
       "1               1.8          34.1                      1.1            8.5   \n",
       "2               5.0          16.8                      2.2            2.4   \n",
       "3               5.5          10.6                      2.1            4.3   \n",
       "4               4.3           1.5                      0.3            NaN   \n",
       "\n",
       "   perc_of_black  perc_of_latino  perc_of_white  total_pop      city_id  \n",
       "0            8.0            53.1           52.0      85599   8589934592  \n",
       "1           18.9            25.8           66.4     128877   8589934593  \n",
       "2            1.0            47.1           84.6      93700   8589934594  \n",
       "3            7.0            32.8           87.3     199651  17179869184  \n",
       "4           15.0             3.6           86.0      69701  17179869185  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_spark_pivot.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_demographics= df_i94port_dict\\\n",
    "    .join(df_demographics_spark_pivot, \n",
    "          (df_i94port_dict['city_name'] == df_demographics_spark_pivot['city']) & (df_i94port_dict['state_name'] == df_demographics_spark_pivot['state']), \"inner\")\\\n",
    "    .drop(\"city_name\", \"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>perc_of_male_pop</th>\n",
       "      <th>perc_of_female_pop</th>\n",
       "      <th>perc_of_veterans</th>\n",
       "      <th>perc_of_born</th>\n",
       "      <th>perc_of_native_american</th>\n",
       "      <th>perc_of_asian</th>\n",
       "      <th>perc_of_black</th>\n",
       "      <th>perc_of_latino</th>\n",
       "      <th>perc_of_white</th>\n",
       "      <th>total_pop</th>\n",
       "      <th>city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PHO</td>\n",
       "      <td>AZ</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>34.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>49.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>42.9</td>\n",
       "      <td>74.3</td>\n",
       "      <td>1563001</td>\n",
       "      <td>326417514497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUC</td>\n",
       "      <td>AZ</td>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>34.0</td>\n",
       "      <td>49.8</td>\n",
       "      <td>50.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>43.5</td>\n",
       "      <td>76.1</td>\n",
       "      <td>531674</td>\n",
       "      <td>472446402561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YUI</td>\n",
       "      <td>AZ</td>\n",
       "      <td>YUMA</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>33.0</td>\n",
       "      <td>51.3</td>\n",
       "      <td>48.7</td>\n",
       "      <td>7.6</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.6</td>\n",
       "      <td>74.0</td>\n",
       "      <td>94145</td>\n",
       "      <td>25769803777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YUM</td>\n",
       "      <td>AZ</td>\n",
       "      <td>YUMA</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>33.0</td>\n",
       "      <td>51.3</td>\n",
       "      <td>48.7</td>\n",
       "      <td>7.6</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.6</td>\n",
       "      <td>74.0</td>\n",
       "      <td>94145</td>\n",
       "      <td>25769803777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHL</td>\n",
       "      <td>SC</td>\n",
       "      <td>CHARLESTON</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>52.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>76.8</td>\n",
       "      <td>135524</td>\n",
       "      <td>850403524608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code state_code        city           state  median_age  \\\n",
       "0       PHO         AZ     PHOENIX         ARIZONA        34.0   \n",
       "1       TUC         AZ      TUCSON         ARIZONA        34.0   \n",
       "2       YUI         AZ        YUMA         ARIZONA        33.0   \n",
       "3       YUM         AZ        YUMA         ARIZONA        33.0   \n",
       "4       CHL         SC  CHARLESTON  SOUTH CAROLINA        35.0   \n",
       "\n",
       "   perc_of_male_pop  perc_of_female_pop  perc_of_veterans  perc_of_born  \\\n",
       "0              50.3                49.7               4.6          19.2   \n",
       "1              49.8                50.2               7.2          15.5   \n",
       "2              51.3                48.7               7.6          20.5   \n",
       "3              51.3                48.7               7.6          20.5   \n",
       "4              47.2                52.8               6.9           4.3   \n",
       "\n",
       "   perc_of_native_american  perc_of_asian  perc_of_black  perc_of_latino  \\\n",
       "0                      2.7            4.2            8.5            42.9   \n",
       "1                      4.6            4.6            6.4            43.5   \n",
       "2                      1.3            1.3            4.0            60.6   \n",
       "3                      1.3            1.3            4.0            60.6   \n",
       "4                      0.5            2.0           22.1             2.9   \n",
       "\n",
       "   perc_of_white  total_pop       city_id  \n",
       "0           74.3    1563001  326417514497  \n",
       "1           76.1     531674  472446402561  \n",
       "2           74.0      94145   25769803777  \n",
       "3           74.0      94145   25769803777  \n",
       "4           76.8     135524  850403524608  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_demographics.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_demographics_spark = df_demographics_spark.filter(df_demographics_spark['Count'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('median_age', 'float'),\n",
       " ('perc_of_male_pop', 'double'),\n",
       " ('perc_of_female_pop', 'double'),\n",
       " ('perc_of_veterans', 'double'),\n",
       " ('perc_of_born', 'double'),\n",
       " ('perc_of_native_american', 'double'),\n",
       " ('perc_of_asian', 'double'),\n",
       " ('perc_of_black', 'double'),\n",
       " ('perc_of_latino', 'double'),\n",
       " ('perc_of_white', 'double'),\n",
       " ('total_pop', 'int'),\n",
       " ('city_id', 'bigint')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_spark_pivot.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_spark_pivot.createOrReplaceTempView(\"demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------------+-------------+---------+\n",
      "|          city|  state|perc_of_male_pop|perc_of_asian|total_pop|\n",
      "+--------------+-------+----------------+-------------+---------+\n",
      "|      AVONDALE|ARIZONA|            48.0|          3.5|    80683|\n",
      "|  CASAS ADOBES|ARIZONA|            47.3|          4.6|    65265|\n",
      "|      CHANDLER|ARIZONA|            49.4|         12.7|   260833|\n",
      "|     FLAGSTAFF|ARIZONA|            47.2|          3.8|    70317|\n",
      "|       GILBERT|ARIZONA|            47.2|          8.0|   247523|\n",
      "|      GLENDALE|ARIZONA|            48.6|          6.3|   240114|\n",
      "|      GOODYEAR|ARIZONA|            46.5|          7.2|    79003|\n",
      "|          MESA|ARIZONA|            49.8|          3.1|   471833|\n",
      "|       PHOENIX|ARIZONA|            50.3|          4.2|  1563001|\n",
      "|SAN TAN VALLEY|ARIZONA|            48.8|          1.7|    82797|\n",
      "|    SCOTTSDALE|ARIZONA|            48.9|          4.9|   236844|\n",
      "|      SURPRISE|ARIZONA|            49.0|          2.5|   128442|\n",
      "|         TEMPE|ARIZONA|            52.0|          9.9|   175826|\n",
      "|        TUCSON|ARIZONA|            49.8|          4.6|   531674|\n",
      "|          YUMA|ARIZONA|            51.3|          1.3|    94145|\n",
      "+--------------+-------+----------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select city, state, perc_of_male_pop, perc_of_asian, total_pop from demographics where state = 'ARIZONA' order by city \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               state|state_pop|\n",
      "+--------------------+---------+\n",
      "|             ALABAMA|  1049629|\n",
      "|              ALASKA|   298695|\n",
      "|             ARIZONA|  4328300|\n",
      "|            ARKANSAS|   507047|\n",
      "|          CALIFORNIA| 24301460|\n",
      "|            COLORADO|  2463682|\n",
      "|         CONNECTICUT|   797098|\n",
      "|DISTRICT OF COLUMBIA|   672228|\n",
      "|             FLORIDA|  5928707|\n",
      "|             GEORGIA|  1711032|\n",
      "|              HAWAII|   352766|\n",
      "|               IDAHO|   398883|\n",
      "|            ILLINOIS|  4484017|\n",
      "|             INDIANA|  1882753|\n",
      "|                IOWA|   733811|\n",
      "|              KANSAS|   997013|\n",
      "|            KENTUCKY|   929877|\n",
      "|           LOUISIANA|  1172934|\n",
      "|            MARYLAND|  1208662|\n",
      "|       MASSACHUSETTS|  1780881|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select state,  sum(total_pop) as state_pop from demographics group by state order by state\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = pd.read_csv('data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country',\n",
       "       'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code',\n",
       "       'coordinates'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27133"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport['municipality'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49399"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport['municipality'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport['type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            object\n",
       "type             object\n",
       "name             object\n",
       "elevation_ft    float64\n",
       "continent        object\n",
       "iso_country      object\n",
       "iso_region       object\n",
       "municipality     object\n",
       "gps_code         object\n",
       "iata_code        object\n",
       "local_code       object\n",
       "coordinates      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_spark = spark.read \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\",True) \\\n",
    "  .csv(\"data/airport-codes_csv.csv\") \\\n",
    "  .drop(\"coordinates\", \"gps_code\", \"continent\", \"elevation_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45886"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_spark.filter(df_airport_spark[\"iata_code\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "|ident|         type|                name|iso_country|iso_region|municipality|iata_code|local_code|\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "|  00A|     heliport|   Total Rf Heliport|         US|     US-PA|    Bensalem|     null|       00A|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|         US|     US-KS|       Leoti|     null|      00AA|\n",
      "| 00AK|small_airport|        Lowell Field|         US|     US-AK|Anchor Point|     null|      00AK|\n",
      "| 00AL|small_airport|        Epps Airpark|         US|     US-AL|     Harvest|     null|      00AL|\n",
      "| 00AR|       closed|Newport Hospital ...|         US|     US-AR|     Newport|     null|      null|\n",
      "| 00AS|small_airport|      Fulton Airport|         US|     US-OK|        Alex|     null|      00AS|\n",
      "| 00AZ|small_airport|      Cordes Airport|         US|     US-AZ|      Cordes|     null|      00AZ|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|         US|     US-CA|     Barstow|     null|      00CA|\n",
      "| 00CL|small_airport| Williams Ag Airport|         US|     US-CA|       Biggs|     null|      00CL|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|         US|     US-CA| Pine Valley|     null|      00CN|\n",
      "| 00CO|       closed|          Cass Field|         US|     US-CO|  Briggsdale|     null|      null|\n",
      "| 00FA|small_airport| Grass Patch Airport|         US|     US-FL|    Bushnell|     null|      00FA|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|         US|     US-FL|   Riverview|     null|      00FD|\n",
      "| 00FL|small_airport|   River Oak Airport|         US|     US-FL|  Okeechobee|     null|      00FL|\n",
      "| 00GA|small_airport|    Lt World Airport|         US|     US-GA|    Lithonia|     null|      00GA|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         US|     US-GA|       Hiram|     null|      00GE|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|         US|     US-HI| Kailua/Kona|     null|      00HI|\n",
      "| 00ID|small_airport|Delta Shores Airport|         US|     US-ID|  Clark Fork|     null|      00ID|\n",
      "| 00IG|small_airport|       Goltl Airport|         US|     US-KS|    McDonald|     null|      00IG|\n",
      "| 00II|     heliport|Bailey Generation...|         US|     US-IN|  Chesterton|     null|      00II|\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_spark.select(\"ident\",\"type\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_spark=df_airport_spark\\\n",
    "    .filter((col(\"type\")==\"small_airport\") | (col(\"type\")==\"large_airport\") | (col(\"type\")==\"medium_airport\"))\\\n",
    "    .filter(df_airport_spark[\"iso_country\"]==\"US\")\\\n",
    "    .filter(df_airport_spark['local_code'].isNotNull())\\\n",
    "    .withColumn(\"state\",substring(df_airport_spark[\"iso_region\"],4,2))\n",
    "\n",
    "# df_airport_spark=df_airport_spark\\\n",
    "#     .filter(df_airport_spark[\"type\"]==\"small_airport\")\\\n",
    "#     .filter(df_airport_spark[\"iso_country\"]==\"US\")\\\n",
    "#     .filter(df_airport_spark['iata_code'].isNotNull())\\\n",
    "#     .withColumn(\"state\",substring(df_airport_spark[\"iso_region\"],4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality       12\n",
       "iata_code       12522\n",
       "local_code          0\n",
       "state               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_spark.toPandas().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+-----+\n",
      "|ident|         type|                name|iso_country|iso_region|municipality|iata_code|local_code|state|\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+-----+\n",
      "| 00AA|small_airport|Aero B Ranch Airport|         US|     US-KS|       Leoti|     null|      00AA|   KS|\n",
      "| 00AK|small_airport|        Lowell Field|         US|     US-AK|Anchor Point|     null|      00AK|   AK|\n",
      "| 00AL|small_airport|        Epps Airpark|         US|     US-AL|     Harvest|     null|      00AL|   AL|\n",
      "| 00AS|small_airport|      Fulton Airport|         US|     US-OK|        Alex|     null|      00AS|   OK|\n",
      "| 00AZ|small_airport|      Cordes Airport|         US|     US-AZ|      Cordes|     null|      00AZ|   AZ|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|         US|     US-CA|     Barstow|     null|      00CA|   CA|\n",
      "| 00CL|small_airport| Williams Ag Airport|         US|     US-CA|       Biggs|     null|      00CL|   CA|\n",
      "| 00FA|small_airport| Grass Patch Airport|         US|     US-FL|    Bushnell|     null|      00FA|   FL|\n",
      "| 00FL|small_airport|   River Oak Airport|         US|     US-FL|  Okeechobee|     null|      00FL|   FL|\n",
      "| 00GA|small_airport|    Lt World Airport|         US|     US-GA|    Lithonia|     null|      00GA|   GA|\n",
      "| 00ID|small_airport|Delta Shores Airport|         US|     US-ID|  Clark Fork|     null|      00ID|   ID|\n",
      "| 00IG|small_airport|       Goltl Airport|         US|     US-KS|    McDonald|     null|      00IG|   KS|\n",
      "| 00IL|small_airport|      Hammer Airport|         US|     US-IL|        Polo|     null|      00IL|   IL|\n",
      "| 00IS|small_airport|Hayenga's Cant Fi...|         US|     US-IL|       Kings|     null|      00IS|   IL|\n",
      "| 00KS|small_airport| Hayden Farm Airport|         US|     US-KS|     Gardner|     null|      00KS|   KS|\n",
      "| 00KY|small_airport|Robbins Roost Air...|         US|     US-KY|    Stanford|     null|      00KY|   KY|\n",
      "| 00LS|small_airport|     Lejeune Airport|         US|     US-LA|   Esterwood|     null|      00LS|   LA|\n",
      "| 00MD|small_airport|        Slater Field|         US|     US-MD|Federalsburg|     null|      00MD|   MD|\n",
      "| 00MN|small_airport|Battle Lake Munic...|         US|     US-MN| Battle Lake|     null|      00MN|   MN|\n",
      "| 00MO|small_airport|Cooper Flying Ser...|         US|     US-MO|        Alba|     null|      00MO|   MO|\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_spark=df_airport_spark\\\n",
    ".select(\"local_code\", \"type\", \"name\", upper(col(\"municipality\")).alias(\"city\"), col(\"state\").alias(\"state_code\"), \"iso_country\", \"iata_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_code</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>state_code</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iata_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>LEOTI</td>\n",
       "      <td>KS</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>ANCHOR POINT</td>\n",
       "      <td>AK</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>HARVEST</td>\n",
       "      <td>AL</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>ALEX</td>\n",
       "      <td>OK</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>CORDES</td>\n",
       "      <td>AZ</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  local_code           type                  name          city state_code  \\\n",
       "0       00AA  small_airport  Aero B Ranch Airport         LEOTI         KS   \n",
       "1       00AK  small_airport          Lowell Field  ANCHOR POINT         AK   \n",
       "2       00AL  small_airport          Epps Airpark       HARVEST         AL   \n",
       "3       00AS  small_airport        Fulton Airport          ALEX         OK   \n",
       "4       00AZ  small_airport        Cordes Airport        CORDES         AZ   \n",
       "\n",
       "  iso_country iata_code  \n",
       "0          US      None  \n",
       "1          US      None  \n",
       "2          US      None  \n",
       "3          US      None  \n",
       "4          US      None  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_spark.createOrReplaceTempView(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94port_dict_with_temp.createOrReplaceTempView(\"ports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT iata_code)|\n",
      "+-------------------------+\n",
      "|                     1861|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(iata_code)) from airport\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT port_code)|\n",
      "+-------------------------+\n",
      "|                      525|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(port_code)) from ports\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     224|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from ports join airport on ports.port_code == airport.iata_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+----------+-----------+---------+\n",
      "|ident|          type|      name|      city|state_code|iso_country|iata_code|\n",
      "+-----+--------------+----------+----------+----------+-----------+---------+\n",
      "| KFMY|medium_airport|Page Field|FORT MYERS|        FL|         US|      FMY|\n",
      "+-----+--------------+----------+----------+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from airport WHERE iata_code ='FMY'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+----------+-----------+---------+\n",
      "|ident|type|name|city|state_code|iso_country|iata_code|\n",
      "+-----+----+----+----+----------+-----------+---------+\n",
      "+-----+----+----+----+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from airport WHERE ident ='FMY'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------+-----------+\n",
      "|ident|                name|port_code|    city|iso_country|\n",
      "+-----+--------------------+---------+--------+-----------+\n",
      "| KL06|Furnace Creek Air...|      DTH|    null|         US|\n",
      "| KPIE|St Petersburg Cle...|      PIE|    null|         US|\n",
      "| KADW|  Joint Base Andrews|      ADW|    null|         US|\n",
      "| KNYL|Yuma MCAS/Yuma In...|      YUM|    null|         US|\n",
      "| KORL|Orlando Executive...|      ORL| ORLANDO|         US|\n",
      "| KSAV|Savannah Hilton H...|      SAV|SAVANNAH|         US|\n",
      "| KCAR|Caribou Municipal...|      CAR|    null|         US|\n",
      "| KUGN|Waukegan National...|      UGN|    null|         US|\n",
      "| KPGR|          Kirk Field|      PGR|    null|         US|\n",
      "| KSAR|Sparta Community ...|      SAR|    null|         US|\n",
      "| KATL|Hartsfield Jackso...|      ATL| ATLANTA|         US|\n",
      "| KMIA|Miami Internation...|      MIA|   MIAMI|         US|\n",
      "| PAHO|       Homer Airport|      HOM|    null|         US|\n",
      "| KMEM|Memphis Internati...|      MEM| MEMPHIS|         US|\n",
      "| KGGW|Wokal Field Glasg...|      GGW|    null|         US|\n",
      "| KELY|Ely Airport Yella...|      ELY|    null|         US|\n",
      "| KPOE| Polk Army Air Field|      POE|    null|         US|\n",
      "| KMOB|Mobile Regional A...|      MOB|  MOBILE|         US|\n",
      "| KRFD|Chicago Rockford ...|      RFD|    null|         US|\n",
      "| KHOP|Campbell AAF (For...|      HOP|    null|         US|\n",
      "+-----+--------------------+---------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT a.ident, a.name, p.port_code,  p.city, iso_country\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.iata_code = p.port_code\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   57|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT count(DISTINCT(a.ident)) as count\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.iata_code = p.port_code\n",
    "     WHERE p.city != \"INVALID ENTRY\" and iso_country = \"US\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  584|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT count(DISTINCT(a.ident)) as count\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.city = p.city_name\n",
    "     WHERE p.city != \"INVALID ENTRY\" and iso_country = \"US\" \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "     SELECT a.ident, a.name, p.port_code,  p.city, iso_country, iata_code\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.city = p.city_name\n",
    "     WHERE iso_country = \"US\" \n",
    "     order by p.port_code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1)\\\n",
    ".write.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(\"../data/output/mydata2.csv\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"\"\"\n",
    "     SELECT *\n",
    "     FROM ports \n",
    "     order by port_code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.repartition(1)\\\n",
    ".write.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(\"../data/output/mydata3.csv\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT DISTINCT(type) \n",
    "     FROM airport\n",
    "     WHERE city != \"INVALID ENTRY\" and iso_country = \"US\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT count(DISTINCT(a.ident)) as count\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.ident = p.port_code\n",
    "     WHERE p.city != \"INVALID ENTRY\" and iso_country = \"US\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------+---------+---------+\n",
      "|ident|city|iso_country|port_code|iata_code|\n",
      "+-----+----+-----------+---------+---------+\n",
      "|  48Y|null|         US|      48Y|     null|\n",
      "|  MOS|null|         US|      MOS|     null|\n",
      "|  ORI|null|         US|      ORI|     null|\n",
      "|  SAS|null|         US|      SAS|      SAS|\n",
      "+-----+----+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT ident, p.city, iso_country, port_code, iata_code\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.ident = p.port_code\n",
    "     WHERE iso_country = \"US\"\n",
    "     order by 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+---------+\n",
      "|ident|       city|iso_country|port_code|iata_code|\n",
      "+-----+-----------+-----------+---------+---------+\n",
      "| 89NY|       null|         US|      AXB|      AXB|\n",
      "| K5T9|       null|         US|      EGP|      EGP|\n",
      "| KABE|       null|         US|      ABE|      ABE|\n",
      "| KABQ|ALBUQUERQUE|         US|      ABQ|      ABQ|\n",
      "| KACY|       null|         US|      ACY|      ACY|\n",
      "| KADH|       null|         US|      ADT|      ADT|\n",
      "| KADS|       null|         US|      ADS|      ADS|\n",
      "| KADW|       null|         US|      ADW|      ADW|\n",
      "| KAFW|       null|         US|      AFW|      AFW|\n",
      "| KAGS|       null|         US|      AGS|      AGS|\n",
      "| KALB|       null|         US|      ALB|      ALB|\n",
      "| KAND|       null|         US|      AND|      AND|\n",
      "| KANP|       null|         US|      ANP|      ANP|\n",
      "| KAPA|       null|         US|      APA|      APA|\n",
      "| KAPF|       null|         US|      APF|      APF|\n",
      "| KASE|       null|         US|      ASE|      ASE|\n",
      "| KAST|       null|         US|      AST|      AST|\n",
      "| KATL|    ATLANTA|         US|      ATL|      ATL|\n",
      "| KAUS|     AUSTIN|         US|      AUS|      AUS|\n",
      "| KBBP|BATON ROUGE|         US|      BTN|      BTN|\n",
      "+-----+-----------+-----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     SELECT ident, p.city, iso_country, port_code, iata_code\n",
    "     FROM airport a\n",
    "     JOIN ports p ON a.iata_code = p.port_code\n",
    "     WHERE  iso_country = \"US\"\n",
    "     order by 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Temperature Data Summary: \n",
    "1. First column df should have a meaningful name\n",
    "2. AverageTemperature and AverageTemperatureUncertainty column have NaN record\n",
    "3. Data type for date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The star schema is chosen as the data model because it is simple and yet effective. users can write simple queries by joing fact and dimension tables to analyze the data.\n",
    "\n",
    "Here are the tables of the schema:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact Table: fact_immigration**\n",
    "- id                  (PK)\n",
    "- arr_ts              (FK)\n",
    "- arrival_year         \n",
    "- arrival_month\n",
    "- country_of_bir      (FK)\n",
    "- country_of_res      (FK)\n",
    "- port_of_admission   (FK)\n",
    "- mode\n",
    "- visa_category\n",
    "- visatype\n",
    "- dep_ts              (FK)\n",
    "\n",
    "**Dimension Tables: dim_immigrant**   \n",
    "- immigrant_id         (PK)\n",
    "- gender\n",
    "- age\n",
    "- birth_year\n",
    "\n",
    "**Dimension Tables: dim_country**  \n",
    "- country_code         (PK)\n",
    "- country_name\n",
    "\n",
    "**Dimension Tables: dim_time**  \n",
    "- time_id              (PK)\n",
    "- date  \n",
    "- year\n",
    "- quarter\n",
    "- month\n",
    "- day\n",
    "- weekday\n",
    "- week\n",
    "**Dimension Tables: dim_demographics**   \n",
    "- port_code            (PK)\n",
    "- state_code\n",
    "- city\n",
    "- state\n",
    "- median_age\n",
    "- perc_of_male_pop\n",
    "- perc_of_female_pop\n",
    "- perc_of_veterans\n",
    "- perc_of_foreign_born\n",
    "- perc_of_native_american\n",
    "- perc_of_asian\n",
    "- perc_of_black\n",
    "- perc_of_latin\n",
    "- perc_of_white\n",
    "- total_pop\n",
    "**Dimension Tables: dim_airport**   \n",
    "- local_code            (PK)\n",
    "- airport_name\n",
    "- airport_type\n",
    "- state_code\n",
    "- state\n",
    "- iso_country\n",
    "- iata_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function processes the songs data files.\n",
    "    1. Read and load immigration_data from S3\n",
    "    2. Transform them to create immigration\n",
    "    3. Write them to partioned parquet files in table directories on S3\n",
    "\n",
    "    :param spark: spark session\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "    :return: dim_immigrant, dim_time, dim_immigration\n",
    "    \"\"\"\n",
    "    print('Processing US immigration dataset...')\n",
    "\n",
    "    # paths of input datasets\n",
    "    months = ['jun']\n",
    "    # months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    paths = [month.join(['18-83510-I94-Data-2016/i94_', '16_sub.sas7bdat']) for month in months]\n",
    "    # immigration_data_paths = [os.path.join(input_data, path) for path in paths]\n",
    "    immigration_data_paths = [input_data + path for path in paths]\n",
    "\n",
    "    # Defining User Defined Function (UDF) to convert log timestamp (seconds since epoch) to \\\n",
    "    # actual Datetime Type timestamp\n",
    "    # create timestamp column from original timestamp column\n",
    "\n",
    "    # convert_sas_date = udf(lambda days: (datetime.date(1960, 1, 1) + datetime.timedelta(days=days)), T.DateType())\n",
    "    # df = df.withColumn('timestamp', get_timestamp(df.ts))\n",
    "\n",
    "    convert_i94mode_udf = udf(convert_i94mode, StringType())\n",
    "    convert_sas_date_udf = udf(convert_sas_date, DateType())\n",
    "    convert_visa_udf = udf(convert_visa, StringType())\n",
    "    # get_sas_day_udf = udf(get_sas_day, IntegerType())\n",
    "\n",
    "    # immigration_data_paths = os.path.join(input_data, '18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat')\n",
    "    for immigration_data_paths in immigration_data_paths:\n",
    "        # get filepath to song data file\n",
    "        print(\"Processing fact_immigration spark dataframe\")\n",
    "        df_immigration_spark = spark.read\\\n",
    "            .format('com.github.saurfang.sas.spark')\\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .load(immigration_data_paths)\n",
    "\n",
    "    #    .schema(schema['immigration_schema'])\n",
    "\n",
    "        df_immigration_spark = df_immigration_spark \\\n",
    "            .withColumn('arrival_date', convert_sas_date_udf(df_immigration_spark['arrdate'])) \\\n",
    "            .withColumn('departure_date', convert_sas_date_udf(df_immigration_spark['depdate'])) \\\n",
    "            .withColumn('arrival_year', df_immigration_spark['i94yr'].cast(IntegerType())) \\\n",
    "            .withColumn('arrival_month', df_immigration_spark['i94mon'].cast(IntegerType())) \\\n",
    "            .withColumn('age', df_immigration_spark['i94bir'].cast(IntegerType())) \\\n",
    "            .withColumn('country_of_bir', df_immigration_spark['i94cit'].cast(IntegerType())) \\\n",
    "            .withColumn('country_of_res', df_immigration_spark['i94res'].cast(IntegerType())) \\\n",
    "            .withColumn('port_of_admission', df_immigration_spark['i94port'].cast(StringType())) \\\n",
    "            .withColumn('birth_year', df_immigration_spark['biryear'].cast(IntegerType())) \\\n",
    "            .withColumn('mode', convert_i94mode_udf(df_immigration_spark['i94mode'])) \\\n",
    "            .withColumn('visa_category', convert_visa_udf(df_immigration_spark['i94visa']))\n",
    "\n",
    "    #    .withColumn('arrival_day', get_sas_day_udf(df_immigration_spark['arrdate'])) \\\n",
    "\n",
    "        dim_immigration = df_immigration_spark.select(\n",
    "            col('cicid').alias('id'),\n",
    "            col('arrdate').alias('arr_ts'),\n",
    "            'arrival_year',\n",
    "            'arrival_month',\n",
    "            'country_of_bir',\n",
    "            'country_of_res',\n",
    "            'port_of_admission',\n",
    "            'mode',\n",
    "            'visa_category',\n",
    "            'visatype',\n",
    "            col('depdate').alias('dep_ts')) \\\n",
    "            .dropDuplicates()\n",
    "\n",
    "        # immigration_out_path = os.path.join(output_data, 'fact_immigration/')\n",
    "        immigration_out_path = output_data + 'fact_immigration/'\n",
    "\n",
    "        dim_immigration.write.parquet(immigration_out_path, mode='append', partitionBy=('arrival_year',\n",
    "                                                                                        'arrival_month'))\n",
    "\n",
    "        df_time_spark = df_immigration_spark.select(col(\"arrdate\").alias(\"time\")).union(\n",
    "            df_immigration_spark.select(col(\"depdate\").alias(\"time\"))).distinct().na.drop()\n",
    "\n",
    "        # Create time dimension spark dateframe\n",
    "        print(\"Processing time dimension spark dataframe\")\n",
    "        dim_time = df_time_spark.withColumn('date', convert_sas_date_udf(df_time_spark['time']))\n",
    "\n",
    "        dim_time = dim_time\\\n",
    "            .select(\n",
    "                col('time').alias('time_id')\n",
    "                , col('date').alias('date')\n",
    "                , year('date').alias('year')\n",
    "                , quarter('date').alias('quarter')\n",
    "                , month('date').alias('month')\n",
    "                , dayofmonth('date').alias('day')\n",
    "                , dayofweek('date').alias('weekday')\n",
    "                , weekofyear('date').alias('week'))\n",
    "        # time_out_path = os.path.join(output_data, 'dim_time/')\n",
    "        time_out_path = output_data + 'dim_time/'\n",
    "        dim_time.write.parquet(time_out_path, mode='append', partitionBy='year')\n",
    "\n",
    "        # Create immigrant dimension spark dateframe\n",
    "        print(\"Processing immigrant dimension spark dataframe\")\n",
    "        dim_immigrant = df_immigration_spark \\\n",
    "            .filter(col('birth_year') >= 1900) \\\n",
    "            .filter(col('birth_year') <= 2020) \\\n",
    "            .select(\n",
    "                col('cicid').alias('immigrant_id'),\n",
    "                'gender',\n",
    "                'age',\n",
    "                'birth_year') \\\n",
    "            .dropDuplicates()\n",
    "        # immigrant_out_path = os.path.join(output_data, 'dim_immigrant/')\n",
    "        immigrant_out_path = output_data + 'dim_immigrant/'\n",
    "        dim_immigrant.write.parquet(immigrant_out_path, mode='append', partitionBy='birth_year')\n",
    "\n",
    "    print('Finished processing us immigration data.')\n",
    "    # return dim_immigrant, dim_time, dim_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_demographics_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Process the demographics data files, transform it to demographics table\n",
    "    :param spark: spark session\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "    \"\"\"\n",
    "    # dim_country, dim_port\n",
    "    dim_port_dict = process_label_data(spark, output_data)\n",
    "    print('Processing us cities demographics dataset...')\n",
    "    # get filepath to demographics data file\n",
    "    # demographics_data_path = os.path.join(input_data, 'us-cities-demographics.csv')\n",
    "    demographics_data_path = input_data + 'us-cities-demographics.csv'\n",
    "    df_demographics_spark = spark.read \\\n",
    "        .format('csv') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .load(demographics_data_path)\n",
    "\n",
    "    # Calculate percentages of each numeric column and create new columns.\n",
    "    df_demographics_spark = df_demographics_spark \\\n",
    "        .withColumn(\"Median Age\", col(\"Median Age\").cast(\"float\")) \\\n",
    "        .withColumn(\"pct_male_pop\",\n",
    "                    df_demographics_spark[\"Male Population\"] / df_demographics_spark[\"Total Population\"] * 100) \\\n",
    "        .withColumn(\"pct_female_pop\",\n",
    "                    df_demographics_spark[\"Female Population\"] / df_demographics_spark[\"Total Population\"] * 100) \\\n",
    "        .withColumn(\"pct_veterans\",\n",
    "                    df_demographics_spark[\"Number of Veterans\"] / df_demographics_spark[\"Total Population\"] * 100) \\\n",
    "        .withColumn(\"pct_foreign_born\",\n",
    "                    df_demographics_spark[\"Foreign-born\"] / df_demographics_spark[\"Total Population\"] * 100) \\\n",
    "        .withColumn(\"pct_race\", df_demographics_spark[\"Count\"] / df_demographics_spark[\"Total Population\"] * 100) \\\n",
    "        .orderBy(\"State\")\n",
    "\n",
    "    # Select columns with new calculated percentages.\n",
    "    df_demographics_spark_select = df_demographics_spark \\\n",
    "        .select(col(\"City\").alias(\"city\"),\n",
    "                col(\"State\").alias(\"state\"),\n",
    "                col(\"Median Age\").alias(\"median_age\"),\n",
    "                \"pct_male_pop\",\n",
    "                \"pct_female_pop\",\n",
    "                \"pct_veterans\",\n",
    "                \"pct_foreign_born\",\n",
    "                \"Race\",\n",
    "                \"pct_race\",\n",
    "                \"Total Population\")\n",
    "\n",
    "    # pivot the Race column\n",
    "    df_demographics_spark_pivot = df_demographics_spark_select\\\n",
    "        .groupBy(\"city\", \"state\", \"median_age\", \"pct_male_pop\", \"pct_female_pop\", \"pct_veterans\",\n",
    "                \"pct_foreign_born\", \"Total Population\")\\\n",
    "        .pivot(\"Race\")\\\n",
    "        .avg(\"pct_race\")\n",
    "\n",
    "    # change the header name of the race fields for spark compatibility and round the percentage.\n",
    "    df_demographics_spark_pivot = df_demographics_spark_pivot \\\n",
    "        .select(upper(col(\"city\")).alias(\"city\"),\n",
    "                upper(col(\"state\")).alias(\"state\"),\n",
    "                round(col(\"median_age\")).alias(\"median_age\"),\n",
    "                round(col(\"pct_male_pop\"), 1).alias(\"perc_of_male_pop\"),\n",
    "                round(col(\"pct_female_pop\"), 1).alias(\"perc_of_female_pop\"),\n",
    "                round(col(\"pct_veterans\"), 1).alias(\"perc_of_veterans\"),\n",
    "                round(col(\"pct_foreign_born\"), 1).alias(\"perc_of_foreign_born\"),\n",
    "                round(col(\"American Indian and Alaska Native\"), 1).alias(\"perc_of_native_american\"),\n",
    "                round(col(\"Asian\"), 1).alias(\"perc_of_asian\"),\n",
    "                round(col(\"Black or African-American\"), 1).alias(\"perc_of_black\"),\n",
    "                round(col(\"Hispanic or Latino\"), 1).alias(\"perc_of_latino\"),\n",
    "                round(col(\"White\"), 1).alias(\"perc_of_white\"),\n",
    "                col(\"Total Population\").alias(\"total_pop\"))\\\n",
    "        .dropDuplicates(['city'])\n",
    "\n",
    "    # dim_demographics = df_demographics_spark_pivot.withColumn(\"city_id\", monotonically_increasing_id())\n",
    "    dim_demographics = dim_port_dict\\\n",
    "        .join(df_demographics_spark_pivot,\n",
    "              (dim_port_dict['city_name'] == df_demographics_spark_pivot['city']) & (\n",
    "                          dim_port_dict['state_name'] == df_demographics_spark_pivot['state']), \"inner\") \\\n",
    "        .drop(\"city_name\", \"state_name\")\n",
    "\n",
    "    # demographics_out_path = os.path.join(output_data, 'dim_demographics/')\n",
    "    demographics_out_path = output_data + 'dim_demographics/'\n",
    "    dim_demographics.write.parquet(demographics_out_path, mode='overwrite', partitionBy='state')\n",
    "    # dim_demographics.write.format(\"csv\").save(\"data/demographics_table/demo.csv\")\n",
    "    print('Finished processing us cities demographics data.')\n",
    "    # return dim_demographics\n",
    "    # port_code/state_code/city/state/median_age/perc_of_male_pop/perc_of_femal......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_airport_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Process the airport data files, transform it to airport table\n",
    "    :param spark: spark session\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "   \"\"\"\n",
    "    print('Processing airport codes dataset...')\n",
    "\n",
    "    # get filepath to airport data file\n",
    "    # airport_data_path = os.path.join(input_data, 'airport-codes_csv.csv')\n",
    "    airport_data_path = input_data + 'airport-codes_csv.csv'\n",
    "    df_airport_spark = spark.read\\\n",
    "        .format('csv')\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .load(airport_data_path)\\\n",
    "        .drop(\"coordinates\", \"gps_code\", \"continent\", \"elevation_ft\")\n",
    "\n",
    "    dim_airport = df_airport_spark\\\n",
    "        .filter((col(\"type\") == \"small_airport\") |\n",
    "                (col(\"type\") == \"large_airport\") |\n",
    "                (col(\"type\") == \"medium_airport\"))\\\n",
    "        .filter(df_airport_spark[\"iso_country\"] == \"US\")\\\n",
    "        .filter(df_airport_spark['local_code'].isNotNull())\\\n",
    "        .withColumn(\"state\", substring(df_airport_spark[\"iso_region\"], 4, 2))\n",
    "#         .filter(df_airport_spark['iata_code'].isNotNull()) \\\n",
    "\n",
    "    # airport_out_path = os.path.join(output_data, 'dim_airport/')\n",
    "    airport_out_path = output_data + 'dim_airport/'\n",
    "    dim_airport.write.parquet(airport_out_path)\n",
    "\n",
    "    print('Finished processing airport codes data.')\n",
    "    # return dim_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Process the temperatureData data files, transform it to temperature table\n",
    "    :param spark: spark session\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "    \"\"\"\n",
    "    print('Processing cities temperature dataset...')\n",
    "    # get filepath to temperatureData data file\n",
    "    # temperature_data_path = os.path.join(input_data, 'GlobalLandTemperaturesByCity.csv')\n",
    "    temperature_data_path = input_data + 'GlobalLandTemperaturesByCity.csv'\n",
    "    df_temperature_spark = spark.read \\\n",
    "        .format('csv') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(temperature_data_path)\n",
    "\n",
    "    df_temperature_spark = df_temperature_spark \\\n",
    "        .filter(df_temperature_spark[\"country\"] == \"United States\") \\\n",
    "        .filter(df_temperature_spark.AverageTemperature.isNotNull())\\\n",
    "        .withColumn(\"year\", year(df_temperature_spark[\"dt\"])) \\\n",
    "        .withColumn(\"month\", month(df_temperature_spark[\"dt\"]))\n",
    "\n",
    "    dim_temperature = df_temperature_spark.select(\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"AverageTemperature\",\n",
    "        \"City\",\n",
    "        \"Country\")\\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    # temperature_out_path = os.path.join(output_data, 'dim_temperature/')\n",
    "    temperature_out_path = output_data + 'dim_temperature/'\n",
    "    dim_temperature.write.parquet(temperature_out_path, mode='overwrite', partitionBy='year')\n",
    "\n",
    "    print('Finished processing cities temperature data.')\n",
    "    # return dim_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 'data/'\n",
    "output_data = '../data_folder/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing i94 SAS dictionary...\n",
      "mapper function\n",
      "data/output1/i94cntyl.csv\n",
      "mapper function\n",
      "data/output1/i94addr.csv\n",
      "mapper function\n",
      "data/output1/i94port.csv\n",
      "Finished processing i94 SAS dictionary\n",
      "Processing us cities demographics dataset...\n",
      "Finished processing us cities demographics data.\n"
     ]
    }
   ],
   "source": [
    "process_demographics_data(spark, input_data, output_data) # dim_demographics, dim_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing airport codes dataset...\n",
      "Finished processing airport codes data.\n"
     ]
    }
   ],
   "source": [
    "process_airport_data(spark, input_data, output_data) # dim_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing US immigration dataset...\n",
      "Processing fact_immigration spark dataframe\n",
      "Processing time dimension spark dataframe\n",
      "Processing immigrant dimension spark dataframe\n",
      "Finished processing us immigration data.\n"
     ]
    }
   ],
   "source": [
    "process_immigration_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data quality...\n",
      "Checking fact_immigration records...\n",
      "Data quality check passed for fact_immigration with 3,287,405 records.\n",
      "Performing key_not_null check on table fact_immigration...\n",
      "Data quality check on fact_immigration passed. All key columns have no null value\n",
      "Finished key_not_null check on table fact_immigration...\n",
      "Checking dim_time records...\n",
      "Data quality check passed for dim_time with 161 records.\n",
      "Performing key_not_null check on table dim_time...\n",
      "Data quality check on dim_time passed. All key columns have no null value\n",
      "Finished key_not_null check on table dim_time...\n",
      "Checking dim_immigrant records...\n",
      "Data quality check passed for dim_immigrant with 3,574,350 records.\n",
      "Checking dim_country records...\n",
      "Data quality check passed for dim_country with 289 records.\n",
      "Checking dim_demographics records...\n",
      "Data quality check passed for dim_demographics with 112 records.\n",
      "Performing key_not_null check on table dim_demographics...\n",
      "Data quality check on dim_demographics passed. All key columns have no null value\n",
      "Finished key_not_null check on table dim_demographics...\n",
      "Checking dim_airport records...\n",
      "Data quality check passed for dim_airport with 14,383 records.\n",
      "Performing key_not_null check on table dim_airport...\n",
      "Data quality check on dim_airport passed. All key columns have no null value\n",
      "Finished key_not_null check on table dim_airport...\n",
      "Finished checking data quality.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Quality Checks. Records for 2016-Jun\n",
    "check_data_quality(spark, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>I94 Immigration Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>cicid</td><td>Unique record ID</td>\n",
    "<tr><td>i94yr</td><td>4 digit year</td>\n",
    "<tr><td>i94mon</td><td>Numeric month</td>\n",
    "<tr><td>i94cit</td><td>3 digit code for immigrant country of birth</td>\n",
    "<tr><td>i94res</td><td>3 digit code for immigrant country of residence </td>\n",
    "<tr><td>i94port</td><td>Port of admission</td>\n",
    "<tr><td>arrdate</td><td>Arrival Date in the USA</td>\n",
    "<tr><td>i94mode</td><td>Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    "<tr><td>i94addr</td><td>USA State of arrival</td>\n",
    "<tr><td>depdate</td><td>Departure Date from the USA</td>\n",
    "<tr><td>i94bir</td><td>Age of Respondent in Years</td>\n",
    "<tr><td>i94visa</td><td>Visa codes collapsed into three categories</td>\n",
    "<tr><td>count</td><td>Field used for summary statistics</td>\n",
    "<tr><td>dtadfile</td><td>Character Date Field - Date added to I-94 Files</td>\n",
    "<tr><td>visapost</td><td>Department of State where where Visa was issued </td>\n",
    "<tr><td>occup</td><td>Occupation that will be performed in U.S</td>\n",
    "<tr><td>entdepa</td><td>Arrival Flag - admitted or paroled into the U.S.</td>\n",
    "<tr><td>entdepd</td><td>Departure Flag - Departed lost I-94 or is deceased</td>\n",
    "<tr><td>entdepu</td><td>Update Flag - Either apprehended overstayed adjusted to perm residence</td>\n",
    "<tr><td>matflag</td><td>Match flag - Match of arrival and departure records</td>\n",
    "<tr><td>biryear</td><td>4 digit year of birth</td>\n",
    "<tr><td>dtaddto</td><td>Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td>\n",
    "<tr><td>gender</td><td>Non-immigrant sex</td>\n",
    "<tr><td>insnum</td><td>INS number</td>\n",
    "<tr><td>airline</td><td>Airline used to arrive in U.S.</td>\n",
    "<tr><td>admnum</td><td>Admission Number</td>\n",
    "<tr><td>fltno</td><td>Flight number of Airline used to arrive in U.S.</td>\n",
    "<tr><td>visatype</td><td>Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Fact_immigration Data Dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>id</td><td>Unique record ID (PK)(FK for dim_immigrant)</td>\n",
    "<tr><td>arr_ts</td><td>Arrival date timestamp (FK for dim_time)</td>\n",
    "<tr><td>arrival_year</td><td>4 digit year (used for partition)</td>\n",
    "<tr><td>arrival_month</td><td>Numeric month (used for partition)</td>\n",
    "<tr><td>country_of_bir</td><td>3 digit code for immigrant country of birth (FK for dim_country)</td>\n",
    "<tr><td>country_of_res</td><td>3 digit code for immigrant country of residence (FK for dim_country) </td>\n",
    "<tr><td>port_of_admission</td><td>I94 port of admission (FK for dim_demographics) </td>\n",
    "<tr><td>mode</td><td>Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    "<tr><td>dep_ts</td><td>Departure date timestamp (FK for dim_time)</td>\n",
    "<tr><td>visa_category</td><td>Visa codes collapsed into three categories</td>\n",
    "<tr><td>visatype</td><td>Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Dim_immigrant Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>immigrant_id</td><td>immigrant id (PK)</td>\n",
    "<tr><td>gender</td><td>Non-immigrant sex</td>\n",
    "<tr><td>age</td><td>Age of Respondent in Years</td>\n",
    "<tr><td>bir_year</td><td>4 digit year of birth</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Dim_country Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>country_code</td><td>3 digit code for country in the world (PK)</td>\n",
    "<tr><td>country_name</td><td>Name of country</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Dim_time Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>time_id</td><td>Timestamp as id (PK)</td>\n",
    "<tr><td>date</td><td>Date type of the timestamp</td>\n",
    "<tr><td>year</td><td>Year of the date</td>\n",
    "<tr><td>quarter</td><td>Quarter of the year</td>\n",
    "<tr><td>month</td><td>Month of the year</td>\n",
    "<tr><td>day</td><td>Day of the month</td>\n",
    "<tr><td>weekday</td><td>Day of the week</td>\n",
    "<tr><td>week</td><td>Week of the year</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Dim_time Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>port_code</td><td>City code as id (PK)</td>\n",
    "<tr><td>state_code</td><td>Two-letter code of the state</td>\n",
    "<tr><td>city</td><td>Year of the date</td>\n",
    "<tr><td>state</td><td>Quarter of the year</td>\n",
    "<tr><td>median_age</td><td>\tMedian age in the city (estimation)</td>\n",
    "<tr><td>perc_of_male_pop</td><td>Percentage of male citizens</td>\n",
    "<tr><td>perc_of_female_pop</td><td>Percentage of female citizens</td>\n",
    "<tr><td>perc_of_veterans</td><td>Percentage of veteran citizens</td>\n",
    "<tr><td>perc_of_foreign_born</td><td>Percentage of citizens born outside of US</td>\n",
    "<tr><td>perc_of_native_american</td><td>Percentage of citizens belonging to this ethnic group</td>\n",
    "<tr><td>perc_of_asian</td><td>Percentage of citizens belonging to this ethnic group</td>\n",
    "<tr><td>perc_of_black</td><td>Percentage of citizens belonging to this ethnic group</td>\n",
    "<tr><td>perc_of_latin</td><td>Percentage of citizens belonging to this ethnic group</td>\n",
    "<tr><td>perc_of_white</td><td>Percentage of citizens belonging to this ethnic group</td>\n",
    "<tr><td>total_pop</td><td>Total number of citizens in the given city</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption><strong>Dim_airport Data dictionary</strong></caption>\n",
    "<thead>\n",
    "<th>Feature</th>\n",
    "<th>Description</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>local_code</td><td>Unique local code of the airport (PK)</td>\n",
    "<tr><td>airport_name</td><td>Name of the Airport</td>\n",
    "<tr><td>airport_type</td><td>Type of the airport</td>\n",
    "<tr><td>state_code</td><td>Two-letter code of the state</td>\n",
    "<tr><td>state</td><td>State where the airport is located</td>\n",
    "<tr><td>iso_country</td><td>Country where the airport is located</td>\n",
    "<tr><td>iata_code</td><td>Code of the airport assigned by International Air Transport Association (PK)</td>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to use technologies that allow to run it both locally and in the cloud. Spark allows us to run this project locally and can be easily promoted to an EMR cluster running on AWS. The parquet files used by Spark provide performance improvement over raw data formats and promise scalability up to many terabytes of data. The following tools are used in my project:\n",
    "- **PySpark/Python/Pandas** - PySpark allows you to interact with the Apache Spark data processing framework by writing Python code. In paricular, PySpark allows you to express data as \"DataFrames,\" which allows you to concentrate on data transformations and other tasks without managing how the dataset is distributed over nodes in the computing cluster.\n",
    "- **Amazon EMR Cluster(Spark/Hadoop)** - The python script is submittted to Amazon's EMR Cluster (1 Master and 2 Core nodes), where it is configured with Spark 2.4.5 and Haddop 2.7 to process the spark job.\n",
    "- **S3** - The raw data and cleaned data is stored on Amazon's S3 bucket. \n",
    "- **Jupyter Notebook** - Jupyter notebook is used to both explore the data and analyze the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Propose how often the data should be updated and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we receive one file per month it seems reasonable to update the model monthly. However, we can also partition the immigration data daily so that one could update the dataset everyday and process the data daily as new data coming in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " - The data was increased by 100x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data were increased 100x, then we should be able to easily handle it using AWS EMR.  \n",
    "The solution can be moved to AWS EMR cluster and the cluster could scale horizontally by adding new nodes. Or the data can be transferred to RedShift over Airflow. Both solutions can scale way above the 300 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The data populates a dashboard that must be updated on a daily basis by 7am every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire ETL process takes up to 1h. Processing new data every morning would not cause any problems. The data could be stored on S3, processed by EMR and deposited to Redshift. Entire process should not exceed 1 hour for the current size of data.\n",
    "We could also create pipeline using Airflow and modify the scheduling to a specific time of day and introduce an SLA in Airflow to ensure jobs are completed in a timely manner and adjust accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data stored on S3 can be loaded to RedShift on AWS, which could then support a wide array of BI tools. Amazon Redshift as a data warehouse can be scaled as needed to support increased number of read requests per unit of time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
